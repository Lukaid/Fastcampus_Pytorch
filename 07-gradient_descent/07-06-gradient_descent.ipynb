{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = torch.FloatTensor([[.1, .2, .3],\n",
    "                            [.4, .5, .6],\n",
    "                            [.7, .8, .9]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[0.1282, 0.2342, 0.0170],\n",
       "        [0.2083, 0.7374, 0.9349],\n",
       "        [0.5477, 0.1604, 0.8555]], requires_grad=True)"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "x = torch.rand_like(target)\n",
    "# This means the final scalar will be differentiate by x.\n",
    "x.requires_grad = True\n",
    "# You can get gradient of x, after differentiation.\n",
    "\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor(0.0802, grad_fn=<MseLossBackward>)"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "loss = F.mse_loss(x, target)\n",
    "\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1-th Loss: 4.8501e-02\ntensor([[0.1219, 0.2266, 0.0799],\n        [0.2509, 0.6846, 0.8605],\n        [0.5815, 0.3025, 0.8654]], requires_grad=True)\n2-th Loss: 2.9340e-02\ntensor([[0.1171, 0.2207, 0.1288],\n        [0.2840, 0.6436, 0.8026],\n        [0.6079, 0.4131, 0.8731]], requires_grad=True)\n3-th Loss: 1.7749e-02\ntensor([[0.1133, 0.2161, 0.1668],\n        [0.3098, 0.6117, 0.7576],\n        [0.6283, 0.4991, 0.8791]], requires_grad=True)\n4-th Loss: 1.0737e-02\ntensor([[0.1103, 0.2125, 0.1964],\n        [0.3298, 0.5869, 0.7226],\n        [0.6443, 0.5659, 0.8837]], requires_grad=True)\n5-th Loss: 6.4952e-03\ntensor([[0.1080, 0.2097, 0.2194],\n        [0.3454, 0.5676, 0.6953],\n        [0.6566, 0.6180, 0.8873]], requires_grad=True)\n6-th Loss: 3.9292e-03\ntensor([[0.1062, 0.2076, 0.2373],\n        [0.3576, 0.5525, 0.6741],\n        [0.6663, 0.6584, 0.8902]], requires_grad=True)\n7-th Loss: 2.3769e-03\ntensor([[0.1049, 0.2059, 0.2513],\n        [0.3670, 0.5409, 0.6577],\n        [0.6738, 0.6899, 0.8923]], requires_grad=True)\n8-th Loss: 1.4379e-03\ntensor([[0.1038, 0.2046, 0.2621],\n        [0.3743, 0.5318, 0.6448],\n        [0.6796, 0.7143, 0.8940]], requires_grad=True)\n9-th Loss: 8.6984e-04\ntensor([[0.1029, 0.2036, 0.2705],\n        [0.3800, 0.5247, 0.6349],\n        [0.6841, 0.7334, 0.8954]], requires_grad=True)\n10-th Loss: 5.2620e-04\ntensor([[0.1023, 0.2028, 0.2771],\n        [0.3845, 0.5192, 0.6271],\n        [0.6877, 0.7482, 0.8964]], requires_grad=True)\n11-th Loss: 3.1832e-04\ntensor([[0.1018, 0.2022, 0.2822],\n        [0.3879, 0.5150, 0.6211],\n        [0.6904, 0.7597, 0.8972]], requires_grad=True)\n12-th Loss: 1.9256e-04\ntensor([[0.1014, 0.2017, 0.2861],\n        [0.3906, 0.5116, 0.6164],\n        [0.6925, 0.7687, 0.8978]], requires_grad=True)\n13-th Loss: 1.1649e-04\ntensor([[0.1011, 0.2013, 0.2892],\n        [0.3927, 0.5090, 0.6128],\n        [0.6942, 0.7756, 0.8983]], requires_grad=True)\n14-th Loss: 7.0468e-05\ntensor([[0.1008, 0.2010, 0.2916],\n        [0.3943, 0.5070, 0.6099],\n        [0.6955, 0.7810, 0.8987]], requires_grad=True)\n15-th Loss: 4.2629e-05\ntensor([[0.1007, 0.2008, 0.2935],\n        [0.3956, 0.5055, 0.6077],\n        [0.6965, 0.7853, 0.8990]], requires_grad=True)\n16-th Loss: 2.5788e-05\ntensor([[0.1005, 0.2006, 0.2949],\n        [0.3966, 0.5043, 0.6060],\n        [0.6973, 0.7885, 0.8992]], requires_grad=True)\n17-th Loss: 1.5600e-05\ntensor([[0.1004, 0.2005, 0.2961],\n        [0.3973, 0.5033, 0.6047],\n        [0.6979, 0.7911, 0.8994]], requires_grad=True)\n18-th Loss: 9.4370e-06\ntensor([[0.1003, 0.2004, 0.2969],\n        [0.3979, 0.5026, 0.6036],\n        [0.6983, 0.7931, 0.8995]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "threshold = 1e-5\n",
    "learning_rate = 1.\n",
    "iter_cnt = 0\n",
    "\n",
    "while loss > threshold:\n",
    "    iter_cnt += 1\n",
    "    \n",
    "    loss.backward() # Calculate gradients.\n",
    "\n",
    "    x = x - learning_rate * x.grad  ## x를 업데이트\n",
    "    # 여기서 x.grad가 loss.backward\n",
    "    \n",
    "    # You don't need to aware this now.\n",
    "    x.detach_()\n",
    "    x.requires_grad_(True)\n",
    "    \n",
    "    loss = F.mse_loss(x, target)\n",
    "    \n",
    "    print('%d-th Loss: %.4e' % (iter_cnt, loss))\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}