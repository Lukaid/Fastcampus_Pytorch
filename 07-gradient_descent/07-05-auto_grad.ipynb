{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch AutoGrad\n",
    "\n",
    "- requires_grad 를 True로 하면 해당 탠서에서 이루어지는 연산들을 추적(track)함\n",
    "\n",
    "- 연산이 완료 되면 .backward() 메소드를 호출하여 모든 gradient를 자동 계산\n",
    "\n",
    "- tensor에 대한 gradient는 .grad속성에 누적됨\n",
    "\n",
    "- .detach()를 호출하여 더이상 추적하지 않도록 할 수 있음\n",
    "\n",
    "- 즉, 내용물은 같지만 requires_grad가 다른 새로운 텐서 만들 수 있음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[1., 2.],\n",
       "        [3., 4.]], requires_grad=True)"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "x = torch.FloatTensor([[1, 2],\n",
    "                       [3, 4]]).requires_grad_(True) # 그레디언트가 필요하니? 얘로 미분할거니?\n",
    "\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[3., 4.],\n        [5., 6.]], grad_fn=<AddBackward0>)\ntensor([[-1.,  0.],\n        [ 1.,  2.]], grad_fn=<SubBackward0>)\ntensor([[-3.,  0.],\n        [ 5., 12.]], grad_fn=<MulBackward0>)\ntensor(14., grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x1 = x + 2\n",
    "x2 = x - 2\n",
    "x3 = x1 * x2\n",
    "y = x3.sum()\n",
    "\n",
    "print(x1)\n",
    "print(x2)\n",
    "print(x3)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.backward() # y는 스칼라\n",
    "# backward requires_grad=True 인 애들을 다 미분한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{gathered}\n",
    "x=\\begin{bmatrix}\n",
    "x_{(1,1)} & x_{(1,2)} \\\\\n",
    "x_{(2,1)} & x_{(2,2)}\n",
    "\\end{bmatrix}\\\\\n",
    "\\\\\n",
    "x_1=x+2 \\\\\n",
    "x_2=x-2 \\\\\n",
    "\\\\\n",
    "\\begin{aligned}\n",
    "x_3&=x_1\\times{x_2} \\\\\n",
    "&=(x+2)(x-2) \\\\\n",
    "&=x^2-4\n",
    "\\end{aligned} \\\\\n",
    "\\\\\n",
    "\\begin{aligned}\n",
    "y&=\\text{sum}(x_3) \\\\\n",
    "&=x_{3,(1,1)}+x_{3,(1,2)}+x_{3,(2,1)}+x_{3,(2,2)}\n",
    "\\end{aligned} \\\\\n",
    "\\\\\n",
    "\\text{x.grad}=\\begin{bmatrix}\n",
    "\\frac{\\partial{y}}{\\partial{x_{(1,1)}}} & \\frac{\\partial{y}}{\\partial{x_{(1,2)}}} \\\\\n",
    "\\frac{\\partial{y}}{\\partial{x_{(2,1)}}} & \\frac{\\partial{y}}{\\partial{x_{(2,2)}}}\n",
    "\\end{bmatrix} \\\\\n",
    "\\\\\n",
    "\\frac{dy}{dx}=2x\n",
    "\\end{gathered}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[2., 4.],\n        [6., 8.]])\n"
     ]
    }
   ],
   "source": [
    "print(x.grad) # backward에 저장된 x를 미분?????"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[1., 2.],\n        [3., 4.]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "None\n",
      "<ipython-input-21-e91536e93bb3>:1: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  print(x3.grad)\n"
     ]
    }
   ],
   "source": [
    "print(x3.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "RuntimeError",
     "evalue": "Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead.",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-a71c6bba3ddf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mx3\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m: Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead."
     ]
    }
   ],
   "source": [
    "x3.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[-3.,  0.],\n",
       "       [ 5., 12.]], dtype=float32)"
      ]
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "x3.detach_().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.FloatTensor([[1, 1],\n",
    "                       [1, 1]]).requires_grad_(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[1., 1.],\n        [1., 1.]], grad_fn=<PowBackward0>)\ntensor([[3., 3.],\n        [3., 3.]], grad_fn=<MulBackward0>)\ntensor([[27., 27.],\n        [27., 27.]], grad_fn=<PowBackward0>)\ntensor(108., grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x1 = x ** 2\n",
    "x2 = x1 * 3\n",
    "x3 = x2 ** 3\n",
    "y = x3.sum()\n",
    "\n",
    "print(x1)\n",
    "print(x2)\n",
    "print(x3)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[162., 162.],\n        [162., 162.]])\n"
     ]
    }
   ],
   "source": [
    "print(x.grad) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python391jvsc74a57bd093d2bee35929e05e44f3a424865844abe2a4ec4d78cb24fe7d4202e6ef37a7f8",
   "display_name": "Python 3.9.1 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "metadata": {
   "interpreter": {
    "hash": "93d2bee35929e05e44f3a424865844abe2a4ec4d78cb24fe7d4202e6ef37a7f8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}